# ðŸ¥ Smart Medical Assistant using Multi-Agent LLM 

## ðŸ“Œ Project Overview
Medical prescriptions are often difficult for patients to understand due to handwritten text, medical abbreviations, complex drug names, and dosage instructions. This can lead to confusion, medication errors, and poor treatment adherence.

The **Smart Medical Assistant** is an AI-powered healthcare support system that helps users interpret medical prescriptions in a clear and patient-friendly manner. The system allows users to upload prescription images and generates simplified explanations, safety guidance, and structured medical information using a **multi-agent Large Language Model (LLM) architecture**.

This project integrates **computer vision, natural language processing, pretrained LLMs, and safety validation mechanisms** to deliver an end-to-end intelligent healthcare assistant.

---

## ðŸŽ¯ Objectives
- Extract text from prescription images using OCR
- Identify key medical entities such as medicines, dosage, and frequency
- Generate patient-friendly explanations using LLMs
- Validate outputs for safety and ethical compliance
- Provide assistive guidance without replacing professional medical advice
- Locate nearby doctors and hospitals based on user queries

---

## ðŸ§  System Architecture (Multi-Agent Design)
The system follows a **multi-agent orchestration approach**, where each agent is responsible for a specialized task.

### ðŸ”¹ Agents Overview
1. **Orchestrator (`src/orchestrate.py`)**
   - The central controller that manages the workflow.
   - Routes user requests (images or text) to the appropriate specialist agent.
   - Classifies user intent (Medical Chat vs. Location Search).

2. **Medical Agent (`src/agents/medic_agent.py`)**
   - **Vision Task**: Handles OCR to extract text from prescription images.
   - **Chat Task**: Uses an LLM to explain medicines, side effects, and usage instructions in simple language.

3. **Location Agent (`src/agents/locate_agent.py`)**
   - Handles requests for finding doctors, clinics, or hospitals.
   - Uses geolocation tools to provide nearby medical recommendations.

4. **Helpers (`src/helpers/`)**
   - `ocr_model.py`: Handles the optical character recognition logic.
   - `chat_llm.py`: Interfaces with the Large Language Model for generating responses.
   - `doc_locate.py`: Logic for searching medical facilities.
   - `validate.py`: Ensures safety and adds disclaimers to the output.

### ðŸ“‚ Project Structure
```text
MediScan/
â”œâ”€â”€ data/                     # Data storage (images, dataset, etc.)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/               # Specialist Agents
â”‚   â”‚   â”œâ”€â”€ medic_agent.py    # Handles OCR & Medical Chat
â”‚   â”‚   â””â”€â”€ locate_agent.py   # Handles Doctor/Hospital Search
â”‚   â”œâ”€â”€ helpers/              # Utility Scripts
â”‚   â”‚   â”œâ”€â”€ chat_llm.py       # LLM Interface
â”‚   â”‚   â”œâ”€â”€ doc_locate.py     # Geolocation Logic
â”‚   â”‚   â”œâ”€â”€ ocr_model.py      # OCR Logic
â”‚   â”‚   â””â”€â”€ validate.py       # Safety Validation
â”‚   â””â”€â”€ orchestrate.py        # Main Controller
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ chat.ipynb            # Notebook for downloading the model
â”‚   â”œâ”€â”€ ocr.ipynb             # Notebook for testing the OCR model
â”‚   â””â”€â”€ location.ipynb        # Notebook for testing the location agent
â”œâ”€â”€ app.py                    # Streamlit Frontend
â”œâ”€â”€ requirements.txt          # Project Dependencies
â””â”€â”€ README.md                 # Project Documentation
```

---

## ðŸ”„ Complete Workflow
1. **Image Upload**: User uploads a prescription image via the Streamlit UI.
2. **Routing**: `app.py` sends the image to the **Orchestrator**.
3. **Processing**: The Orchestrator routes the image to the **Medical Agent**, which uses `ocr_model.py` to extract text.
4. **Interaction**: User asks a question (e.g., "What is this medicine?" or "Find a cardiologist nearby").
5. **Intent Classification**: The Orchestrator determines if the query is **Medical** or **Location-based**.
6. **Response Generation**:
   - **Medical Queries**: Handled by **Medical Agent** (using `chat_llm.py`).
   - **Location Queries**: Handled by **Location Agent** (using `doc_locate.py`).
7. **Validation**: Responses are checked by `validate.py` for safety.
8. **Display**: The final answer is shown to the user in the UI.

![Multi agent LLM Architecture](data/detail_diagram.png)
---



## ðŸ› ï¸ Installation & Usage

### Prerequisites

* **Python 3.10+** (Recommended)
* **Git** installed.
* **Jupyter Notebook** (to run the setup script).
* *(Optional)* NVIDIA GPU with CUDA drivers (for faster local AI performance).

### Installation Steps

1. **Clone the Repository**
```bash
git clone https://github.com/SanjayKumar3110/MediScan.git
cd Mediscan/
```

2. **Create a Virtual Environment**
```bash
python -m venv .venv
# Windows:
.venv\Scripts\activate
# Mac/Linux:
source .venv/bin/activate
```

3. **Install Dependencies**
```bash
pip install -r requirements.txt
```

*Note: If you have an NVIDIA GPU, ensure `llama-cpp-python` is installed with CUDA support for better performance.*
4. **Set Up Environment Variables**
* Create a file named `.env` in the root directory.
* Add your Google Gemini API key (for OCR):

```env
API_KEY_EXTRACTION=your_gemini_api_key_here
```

5. **ðŸ“¥ Download Local Meditron Model**
* Before running the app, you must download the local LLM weights.
1. Navigate to the `test/` folder.
2. Open **`chat.ipynb`** in Jupyter Notebook or VS Code.
3. **Run the first cell** (labeled "Download Model").
4. Wait for the download to complete. This will verify your setup and save the model file (`Meditron3-Gemma2-2B.Q4_K_M.gguf`) into the `models/` directory.

6. **ðŸš€ Run the Application**
```bash
streamlit run app.py
```
---

## ðŸ“– How to Use

### Mode 1: Prescription Analysis (The "Eyes")

1. **Select Mode:** Choose **"Prescription Analysis"** from the sidebar (or let the Orchestrator decide).
2. **Upload:** Drop an image of a medical prescription (JPG/PNG).
3. **Analyze:** Click the **"Analyse Prescription"** button.
* The **OCR Agent** will extract text (Patient Name, Medicines, Dosages).
* The **Safety Agent** will scan for potential drug interactions.


4. **Chat:** Ask questions like *"What is Metformin used for?"* or *"Are there side effects?"*. The local **Medical Agent** will answer privately.

### Mode 2: Find a Doctor (The "Location")

1. **Ask Directly:** You don't need to change menus. Just type in the chat:
* *"Find a cardiologist in Coimbatore"*
* *"Where is the nearest hospital?"*


2. **View Results:** The **Location Agent** will return a list of nearby clinics with Clickable Google Maps Links

---

## ðŸ§© Technologies & Tools Used

### ðŸ”¹ Programming Language

* **Python 3.10+**

### ðŸ”¹ Architecture

* **Multi-Agent System**: Custom Orchestrator design with specialized agents (`MedicalAgent`, `LocationAgent`).

### ðŸ”¹ Frontend / UI
* **Streamlit**: Used for the interactive web interface and session state management.

### ðŸ”¹ AI & Machine Learning

* **Vision & OCR**: **Google Gemini 2.5 Flash** (via `google-generativeai`).
 *Used for*: Handwriting recognition and entity extraction from prescription images.


* **Local LLM (Chat)**: **Meditron3-Gemma2-2B** (GGUF Quantized).
 *Runtime*: **llama.cpp** (via `llama-cpp-python`) for efficient local inference on consumer hardware.
* *Used for*: Private, medically-aware conversation and explanation.



### ðŸ”¹ Geolocation Services

* **OpenStreetMap (Nominatim)**: Used for finding nearby hospitals and specialists without requiring an API key.

### ðŸ”¹ Key Libraries

* `streamlit`: Web UI.
* `google-generativeai`: Google AI Studio SDK.
* `llama-cpp-python`: To run the GGUF model locally with GPU acceleration.
* `huggingface_hub`: To download the quantized Meditron model.
* `python-dotenv`: For secure environment variable management.
* `requests`: For connecting to the Location API.
* `Pillow` (PIL): For image processing.
* `transformers`: For running the GGUF model locally with GPU acceleration.

## âš ï¸ Ethical Considerations & Safety
- The system is designed strictly as an **assistive tool**
- Does **not provide diagnosis or treatment recommendations**
- Explicit medical disclaimers are included
- Encourages users to consult qualified healthcare professionals

---

## ðŸš€ Expected Outcomes
- Improved patient understanding of prescriptions
- Reduced confusion in medication usage
- Demonstration of LLM-driven multi-agent systems in healthcare
- A scalable architecture for future medical AI applications

---

## ðŸ”® Future Enhancements
- Multilingual support
- Voice-based interaction
- Doctor and hospital recommendations using geolocation
- Integration with electronic health records (EHR)
- Mobile application deployment

---

## ðŸ“š Academic Relevance
This project demonstrates:
- Practical application of AI in healthcare
- Multi-agent system design using pretrained models
- Ethical and responsible use of Large Language Models
- End-to-end system integration under real-world constraints

---
## Dataset
The dataset used for training is publicly available on **Kaggle**:
- The project uses the [Doctors Handwritten Prescription BD Dataset](https://www.kaggle.com/datasets/mamun1113/doctors-handwritten-prescription-bd-dataset)


## ðŸ§‘â€ðŸŽ“ Contributors
> - **Sanjay Kumar**
> - **Payas Jenner**
> - **Azim Saleh**
> - **Kabil Doss**

B.Tech â€“ Artificial Intelligence & Data Science  
Final Year Project

---

## ðŸ“„ Disclaimer
This application is intended for educational and assistive purposes only.  
It does not replace professional medical consultation, diagnosis, or treatment.
## License
This project is licensed under the Apache License 2.0.
